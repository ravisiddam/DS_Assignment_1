{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ravisiddam/DS_Assignment_1/blob/master/Untitled8.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "lR-pYbhU1XGV",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "7f032ea1-2cac-4ca8-e7aa-9af85946100f"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded  = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad154004-06d9-4112-9b03-b369a9375997\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ad154004-06d9-4112-9b03-b369a9375997\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving txt_sentoken.zip to txt_sentoken.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W2KjiS_gMLbg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "daf625ea-76ca-41c8-ea6d-103e88088cd4"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load the document\n",
        "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UWhpucyaMfmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "c8b9e698-d59f-4c51-ddf4-546d0cfd46ae"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DHXuF-EOM5EZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "eda1c236-f9fa-4d35-e6ea-4401fdbe16f1"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\t# convert lines to a single blob of text\n",
        "\tdata = '\\n'.join(lines)\n",
        "\t# open file\n",
        "\tfile = open(filename, 'w')\n",
        "\t# write text\n",
        "\tfile.write(data)\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# keep tokens with a min occurrence\n",
        "min_occurrence = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print(len(tokens))\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "44276\n",
            "25767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9eUXcwqlNNdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "befa57ae-d7c2-42da-9636-80d03d2f1307"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "\treturn docs, labels\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# load all training reviews\n",
        "docs, labels = load_clean_dataset(vocab)\n",
        "# summarize what we have\n",
        "print(len(docs), len(labels))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "1800 1800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6il49R6iNa4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "0afcd662-4ea6-4f4b-84f8-d6842f3f9d6c"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "print(Xtrain.shape, Xtest.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "(1800, 25768) (200, 25768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J_1_dxEhN4zs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q pydot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1762ENdOCZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2186
        },
        "outputId": "e821dc72-ef7d-421b-ab39-f1e8a2236063"
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y graphviz\n",
        "!unzip txt_sentoken.zip"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package fontconfig.\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 18408 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fontconfig_2.11.94-0ubuntu2_amd64.deb ...\n",
            "Unpacking fontconfig (2.11.94-0ubuntu2) ...\n",
            "Selecting previously unselected package libjbig0:amd64.\n",
            "Preparing to unpack .../01-libjbig0_2.1-3.1_amd64.deb ...\n",
            "Unpacking libjbig0:amd64 (2.1-3.1) ...\n",
            "Selecting previously unselected package libcdt5.\n",
            "Preparing to unpack .../02-libcdt5_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libcdt5 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libcgraph6.\n",
            "Preparing to unpack .../03-libcgraph6_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libcgraph6 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libtiff5:amd64.\n",
            "Preparing to unpack .../04-libtiff5_4.0.8-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtiff5:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Selecting previously unselected package libwebp6:amd64.\n",
            "Preparing to unpack .../05-libwebp6_0.6.0-3_amd64.deb ...\n",
            "Unpacking libwebp6:amd64 (0.6.0-3) ...\n",
            "Selecting previously unselected package libxpm4:amd64.\n",
            "Preparing to unpack .../06-libxpm4_1%3a3.5.12-1_amd64.deb ...\n",
            "Unpacking libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Selecting previously unselected package libgd3:amd64.\n",
            "Preparing to unpack .../07-libgd3_2.2.5-3_amd64.deb ...\n",
            "Unpacking libgd3:amd64 (2.2.5-3) ...\n",
            "Selecting previously unselected package libpixman-1-0:amd64.\n",
            "Preparing to unpack .../08-libpixman-1-0_0.34.0-1_amd64.deb ...\n",
            "Unpacking libpixman-1-0:amd64 (0.34.0-1) ...\n",
            "Selecting previously unselected package libxcb-render0:amd64.\n",
            "Preparing to unpack .../09-libxcb-render0_1.12-1ubuntu1_amd64.deb ...\n",
            "Unpacking libxcb-render0:amd64 (1.12-1ubuntu1) ...\n",
            "Selecting previously unselected package libxcb-shm0:amd64.\n",
            "Preparing to unpack .../10-libxcb-shm0_1.12-1ubuntu1_amd64.deb ...\n",
            "Unpacking libxcb-shm0:amd64 (1.12-1ubuntu1) ...\n",
            "Selecting previously unselected package libcairo2:amd64.\n",
            "Preparing to unpack .../11-libcairo2_1.14.10-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcairo2:amd64 (1.14.10-1ubuntu1) ...\n",
            "Selecting previously unselected package libltdl7:amd64.\n",
            "Preparing to unpack .../12-libltdl7_2.4.6-2_amd64.deb ...\n",
            "Unpacking libltdl7:amd64 (2.4.6-2) ...\n",
            "Selecting previously unselected package libthai-data.\n",
            "Preparing to unpack .../13-libthai-data_0.1.26-3_all.deb ...\n",
            "Unpacking libthai-data (0.1.26-3) ...\n",
            "Selecting previously unselected package libdatrie1:amd64.\n",
            "Preparing to unpack .../14-libdatrie1_0.2.10-5_amd64.deb ...\n",
            "Unpacking libdatrie1:amd64 (0.2.10-5) ...\n",
            "Selecting previously unselected package libthai0:amd64.\n",
            "Preparing to unpack .../15-libthai0_0.1.26-3_amd64.deb ...\n",
            "Unpacking libthai0:amd64 (0.1.26-3) ...\n",
            "Selecting previously unselected package libpango-1.0-0:amd64.\n",
            "Preparing to unpack .../16-libpango-1.0-0_1.40.12-1_amd64.deb ...\n",
            "Unpacking libpango-1.0-0:amd64 (1.40.12-1) ...\n",
            "Selecting previously unselected package libgraphite2-3:amd64.\n",
            "Preparing to unpack .../17-libgraphite2-3_1.3.10-2_amd64.deb ...\n",
            "Unpacking libgraphite2-3:amd64 (1.3.10-2) ...\n",
            "Selecting previously unselected package libharfbuzz0b:amd64.\n",
            "Preparing to unpack .../18-libharfbuzz0b_1.4.2-1_amd64.deb ...\n",
            "Unpacking libharfbuzz0b:amd64 (1.4.2-1) ...\n",
            "Selecting previously unselected package libpangoft2-1.0-0:amd64.\n",
            "Preparing to unpack .../19-libpangoft2-1.0-0_1.40.12-1_amd64.deb ...\n",
            "Unpacking libpangoft2-1.0-0:amd64 (1.40.12-1) ...\n",
            "Selecting previously unselected package libpangocairo-1.0-0:amd64.\n",
            "Preparing to unpack .../20-libpangocairo-1.0-0_1.40.12-1_amd64.deb ...\n",
            "Unpacking libpangocairo-1.0-0:amd64 (1.40.12-1) ...\n",
            "Selecting previously unselected package libpathplan4.\n",
            "Preparing to unpack .../21-libpathplan4_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libpathplan4 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libgvc6.\n",
            "Preparing to unpack .../22-libgvc6_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libgvc6 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libgvpr2.\n",
            "Preparing to unpack .../23-libgvpr2_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libgvpr2 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libxt6:amd64.\n",
            "Preparing to unpack .../24-libxt6_1%3a1.1.5-1_amd64.deb ...\n",
            "Unpacking libxt6:amd64 (1:1.1.5-1) ...\n",
            "Selecting previously unselected package libxmu6:amd64.\n",
            "Preparing to unpack .../25-libxmu6_2%3a1.1.2-2_amd64.deb ...\n",
            "Unpacking libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Selecting previously unselected package libxaw7:amd64.\n",
            "Preparing to unpack .../26-libxaw7_2%3a1.0.13-1_amd64.deb ...\n",
            "Unpacking libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Selecting previously unselected package graphviz.\n",
            "Preparing to unpack .../27-graphviz_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking graphviz (2.38.0-16ubuntu2) ...\n",
            "Setting up libpathplan4 (2.38.0-16ubuntu2) ...\n",
            "Setting up libxcb-render0:amd64 (1.12-1ubuntu1) ...\n",
            "Setting up libjbig0:amd64 (2.1-3.1) ...\n",
            "Setting up libdatrie1:amd64 (0.2.10-5) ...\n",
            "Setting up libtiff5:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Setting up libgraphite2-3:amd64 (1.3.10-2) ...\n",
            "Setting up libpixman-1-0:amd64 (0.34.0-1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up libltdl7:amd64 (2.4.6-2) ...\n",
            "Setting up libxcb-shm0:amd64 (1.12-1ubuntu1) ...\n",
            "Setting up libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Setting up libxt6:amd64 (1:1.1.5-1) ...\n",
            "Setting up libthai-data (0.1.26-3) ...\n",
            "Setting up libcdt5 (2.38.0-16ubuntu2) ...\n",
            "Setting up fontconfig (2.11.94-0ubuntu2) ...\n",
            "Regenerating fonts cache... done.\n",
            "Setting up libcgraph6 (2.38.0-16ubuntu2) ...\n",
            "Setting up libwebp6:amd64 (0.6.0-3) ...\n",
            "Setting up libcairo2:amd64 (1.14.10-1ubuntu1) ...\n",
            "Setting up libgvpr2 (2.38.0-16ubuntu2) ...\n",
            "Setting up libgd3:amd64 (2.2.5-3) ...\n",
            "Setting up libharfbuzz0b:amd64 (1.4.2-1) ...\n",
            "Setting up libthai0:amd64 (0.1.26-3) ...\n",
            "Setting up libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Setting up libpango-1.0-0:amd64 (1.40.12-1) ...\n",
            "Setting up libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Setting up libpangoft2-1.0-0:amd64 (1.40.12-1) ...\n",
            "Setting up libpangocairo-1.0-0:amd64 (1.40.12-1) ...\n",
            "Setting up libgvc6 (2.38.0-16ubuntu2) ...\n",
            "Setting up graphviz (2.38.0-16ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YIFhoZoGNn32",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "cfcea989-e976-45bf-ed2b-2cc183cfb084"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "\t# define network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile network\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize defined model\n",
        "\tmodel.summary()\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "# define the model\n",
        "n_words = Xtest.shape[1]\n",
        "model = define_model(n_words)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.6918 - acc: 0.6083\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.6839 - acc: 0.8500\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.6670 - acc: 0.8694\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.6404 - acc: 0.9222\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.6051 - acc: 0.9278\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.5647 - acc: 0.9372\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.5211 - acc: 0.9372\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.4768 - acc: 0.9517\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.4339 - acc: 0.9522\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.3934 - acc: 0.9578\n",
            "Test Accuracy: 86.500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-fIOjHEA2O8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1305
        },
        "outputId": "fa7ec32c-ddeb-4969-a8ba-39c82e2d80b6"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "\t# define network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile network\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# evaluate a neural network model\n",
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "\tscores = list()\n",
        "\tn_repeats = 10\n",
        "\tn_words = Xtest.shape[1]\n",
        "\tfor i in range(n_repeats):\n",
        "\t\t# define network\n",
        "\t\tmodel = define_model(n_words)\n",
        "\t\t# fit network\n",
        "\t\tmodel.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
        "\t\t# evaluate\n",
        "\t\t_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "\t\tscores.append(acc)\n",
        "\t\tprint('%d accuracy: %s' % ((i+1), acc))\n",
        "\treturn scores\n",
        "\n",
        "# prepare bag of words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "\t# create the tokenizer\n",
        "\ttokenizer = Tokenizer()\n",
        "\t# fit the tokenizer on the documents\n",
        "\ttokenizer.fit_on_texts(train_docs)\n",
        "\t# encode training data set\n",
        "\tXtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "\t# encode training data set\n",
        "\tXtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "\treturn Xtrain, Xtest\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# run experiment\n",
        "modes = ['binary', 'count', 'tfidf', 'freq']\n",
        "results = DataFrame()\n",
        "for mode in modes:\n",
        "\t# prepare data for mode\n",
        "\tXtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
        "\t# evaluate model on data for mode\n",
        "\tresults[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
        "# summarize results\n",
        "print(results.describe())\n",
        "# plot results\n",
        "results.boxplot()\n",
        "pyplot.show()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "1 accuracy: 0.935\n",
            "2 accuracy: 0.935\n",
            "3 accuracy: 0.93\n",
            "4 accuracy: 0.925\n",
            "5 accuracy: 0.925\n",
            "6 accuracy: 0.925\n",
            "7 accuracy: 0.925\n",
            "8 accuracy: 0.925\n",
            "9 accuracy: 0.93\n",
            "10 accuracy: 0.935\n",
            "1 accuracy: 0.91\n",
            "2 accuracy: 0.9\n",
            "3 accuracy: 0.905\n",
            "4 accuracy: 0.91\n",
            "5 accuracy: 0.91\n",
            "6 accuracy: 0.9\n",
            "7 accuracy: 0.88\n",
            "8 accuracy: 0.895\n",
            "9 accuracy: 0.91\n",
            "10 accuracy: 0.91\n",
            "1 accuracy: 0.875\n",
            "2 accuracy: 0.875\n",
            "3 accuracy: 0.875\n",
            "4 accuracy: 0.845\n",
            "5 accuracy: 0.87\n",
            "6 accuracy: 0.85\n",
            "7 accuracy: 0.86\n",
            "8 accuracy: 0.86\n",
            "9 accuracy: 0.865\n",
            "10 accuracy: 0.87\n",
            "1 accuracy: 0.865\n",
            "2 accuracy: 0.87\n",
            "3 accuracy: 0.86\n",
            "4 accuracy: 0.835\n",
            "5 accuracy: 0.875\n",
            "6 accuracy: 0.865\n",
            "7 accuracy: 0.865\n",
            "8 accuracy: 0.87\n",
            "9 accuracy: 0.865\n",
            "10 accuracy: 0.835\n",
            "          binary      count      tfidf       freq\n",
            "count  10.000000  10.000000  10.000000  10.000000\n",
            "mean    0.929000   0.903000   0.864500   0.860500\n",
            "std     0.004595   0.009775   0.010659   0.014034\n",
            "min     0.925000   0.880000   0.845000   0.835000\n",
            "25%     0.925000   0.900000   0.860000   0.861250\n",
            "50%     0.927500   0.907500   0.867500   0.865000\n",
            "75%     0.933750   0.910000   0.873750   0.868750\n",
            "max     0.935000   0.910000   0.875000   0.875000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFOCAYAAAC8HtVyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGAY4gwAgGGIMwAAhiHOAAAY5v8B+dDfkvexyiUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa32537e668>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gpbASnTHRDRH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "outputId": "5d8c6f12-1cee-4b59-f5e9-eccbccc05d3f"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "\t# define network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile network\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize defined model\n",
        "\tmodel.summary()\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model\n",
        "\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "\t# clean\n",
        "\ttokens = clean_doc(review)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\t# convert to line\n",
        "\tline = ' '.join(tokens)\n",
        "\t# encode\n",
        "\tencoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "\t# predict sentiment\n",
        "\tyhat = model.predict(encoded, verbose=0)\n",
        "\t# retrieve predicted percentage and label\n",
        "\tpercent_pos = yhat[0,0]\n",
        "\tif round(percent_pos) == 0:\n",
        "\t\treturn (1-percent_pos), 'NEGATIVE'\n",
        "\treturn percent_pos, 'POSITIVE'\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab)\n",
        "test_docs, ytest = load_clean_dataset(vocab)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
        "# define network\n",
        "n_words = Xtrain.shape[1]\n",
        "model = define_model(n_words)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# test positive text\n",
        "text = 'Excellent Movie'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_87 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.4460 - acc: 0.8005\n",
            "Epoch 2/10\n",
            " - 2s - loss: 0.0549 - acc: 0.9930\n",
            "Epoch 3/10\n",
            " - 2s - loss: 0.0146 - acc: 1.0000\n",
            "Epoch 4/10\n",
            " - 2s - loss: 0.0061 - acc: 1.0000\n",
            "Epoch 5/10\n",
            " - 2s - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 6/10\n",
            " - 2s - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 7/10\n",
            " - 2s - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 8/10\n",
            " - 2s - loss: 9.0671e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 2s - loss: 6.8402e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 2s - loss: 5.3275e-04 - acc: 1.0000\n",
            "Review: [Excellent Movie]\n",
            "Sentiment: NEGATIVE (54.727%)\n",
            "Review: [This is a bad movie.]\n",
            "Sentiment: NEGATIVE (66.061%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EOxzaDOzo6n5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "7ad5bc96-c5a4-45f7-e773-e2f015ac8b68"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load the document\n",
        "filename = 'cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c0oY4D7PpC-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "4256d151-6ea4-4053-fcd9-16eeadc24e08"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hw-EO8lRpL4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7d9820c2-a844-4f93-d0ac-46998a052f70"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\t# convert lines to a single blob of text\n",
        "\tdata = '\\n'.join(lines)\n",
        "\t# open file\n",
        "\tfile = open(filename, 'w')\n",
        "\t# write text\n",
        "\tfile.write(data)\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# keep tokens with a min occurrence\n",
        "min_occurrence = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print(len(tokens))\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44276\n",
            "25767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "41E-MNJ1pT_8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "1fa8ccab-c5e6-494d-c955-43cd884968cf"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# filter out tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tdocuments = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc(doc, vocab)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "\t# integer encode\n",
        "\tencoded = tokenizer.texts_to_sequences(docs)\n",
        "\t# pad sequences\n",
        "\tpadded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "\treturn padded\n",
        "\n",
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "\tmodel.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "\tmodel.add(MaxPooling1D(pool_size=2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(10, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile network\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize defined model\n",
        "\tmodel.summary()\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load training data\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "# define model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# save the model\n",
        "model.save('model.h5')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 25768\n",
            "Maximum length: 1317\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1317, 100)         2576800   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1310, 32)          25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 655, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 20960)             0         \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 10)                209610    \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,812,053\n",
            "Trainable params: 2,812,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            " - 21s - loss: 0.6958 - acc: 0.5039\n",
            "Epoch 2/10\n",
            " - 18s - loss: 0.6132 - acc: 0.6972\n",
            "Epoch 3/10\n",
            " - 18s - loss: 0.1901 - acc: 0.9356\n",
            "Epoch 4/10\n",
            " - 18s - loss: 0.0186 - acc: 0.9983\n",
            "Epoch 5/10\n",
            " - 18s - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 6/10\n",
            " - 18s - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 7/10\n",
            " - 18s - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 8/10\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}